# -*- coding: utf-8 -*-
"""IntentClassification_ManishaVenkat.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-mDbOo_IYWRt-2UXbFBDzFLembXeC4QT

## **How cohesive are the intents in the dataset? How do we find outliers within an intent category?**

*   TASK: Obtain an intent-level similarity score for each of the intents in the dataset and find outliers in each intent group.

* NOTE: Similarity score in this task measures how cohesive ("related to one another" or "similar in meaning") the utterances within an intent are. The score is from 0-1 (less to more cohesive). If an intent label has a score of 0.700, it means that the utterances in that intent group are more cohesive or similar to one another in meaning than the utterances in an intent group that has a similarity score of 0.300.

* KEY CONCEPTS: Cosine Similarity (See [Section 6.4](https://https://web.stanford.edu/~jurafsky/slp3/6.pdf)), TF-IDF (See [Section 6.5](https://https://web.stanford.edu/~jurafsky/slp3/6.pdf)), [Sentence Embeddings](https://https://sbert.net/examples/training/multilingual/README.html).

* METHODS: In this colab script, I have implemented 2 methods to address this task:

**Method I**: Use TF-IDF weighted vectors as features and compute a similarity/cohesiveness score for an intent.

The function calculate_intent_similarities computes this score. Here is an example similarity score computation for 1 intent with just 3 utterances:

Intent 1: HelpIntent
Utterance 1: was bedeutet das
Utterance 2: wie funktioniert das
Utterance 3: was ich machen kann

TF-IDF converts each utterance into a vector. Words like "was", "verstehe" and "bedeutet" become features.

Then calculate cosine similarity between each pair of utterances. Here are example cosine similarity values:

Utterance 1 vs 2: ~0.75 (similar meaning, some shared words)
Utterance 1 vs 3: ~0.50 (similar meaning, fewer shared words)
Utterance 2 vs 3: ~0.30 (similar meaning, different words)

The similarities are placed in a matrix as such:

[1.00  0.75  0.50]  # Utterance 1 similarities \\
[0.75  1.00  0.30]  # Utterance 2 similarities \\
[0.50  0.30  1.00]  # Utterance 3 similarities \\

Replace self-similarities (1.0 elements in the matrix above shows "utterance 1 similirity to utterance 1" and so on for each utterance) with 0.0 values:

[0.00  0.75  0.50]  
[0.75  0.00  0.30]  
[0.50  0.30  0.00]

Now, get the average similarity within these 3 utterances by adding all elements and dividing by the number of non-diagonal elements (because we want to avoid taking self-similarity into account while averaging):

(0.75 + 0.50 + 0.75 + 0.30 + 0.50 + 0.30) / 6 = 0.516 is the similarity/cohesiveness score of the 'HelpIntent'.

**Method II**: Same as Method 1 but instead of TF-IDF vectors as features, it encodes the entire utterance as a sentence embedding, obtained from a pre-trained Transformer model (SBERT). SBERT is a commonly used multilingual model. Some other models you can use are mentioned in the code comments.

Cosine Similarity (and therefore, the similarity scores) are calculated in the same way as Method 1. What is an improvement in Method 2, compared to Method 1, is that our utterance features in Method 2 capture more nuanced meaning than the features in Method 1 (TF-IDF) because (1) SBERT is trained on a larger dataset than our intent_utterances.csv and (2) it takes the context in which each word ("bedeutet" oder "funktioniert") appears in, instead of just their relative *counts*.

Method 1 is the most intuitive at first glance; but not as generalizable as Method 2. We can discuss this later in a call if you'd like.
"""

!pip install umap-learn

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import pandas as pd
from typing import List, Dict, Tuple
import umap
from umap import UMAP
import plotly.express as px
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, silhouette_samples
import plotly.graph_objects as go
from plotly.subplots import make_subplots

from google.colab import drive
drive.mount('/content/drive')

"""**Initial Data Exploration**"""

df_utts= pd.read_csv('/content/drive/My Drive/intent_utterances.csv')
# print(df_utts.head())

# Getting the unique intents from the data
print(len(df_utts['Intent'].unique())) # 52 different intentions
print(df_utts['Intent'].unique())

# get the distribution of utterances across intents

intent_counts = df_utts['Intent'].value_counts()
total_utterances = len(df_utts)
percentage_share = (intent_counts / total_utterances) * 100

distribution_df = pd.DataFrame({
    'Count': intent_counts,
    'Percentage': percentage_share.round(2)
})

distribution = distribution_df.sort_values('Count', ascending=False)

print(distribution)

"""**METHOD I: TF-IDF Features + Cosine Similarity**"""

class IntentSimilarityScorer:
    def __init__(self, threshold_zscore: float = 2.0):
      # threshold_zscore consider datapoints outside of 2 standard deviations from the mean to be outliers. can change to 1.5 if wanted.
        self.vectorizer = TfidfVectorizer()
        self.threshold_zscore = threshold_zscore

    def calculate_intent_similarities(self, df: pd.DataFrame) -> Dict[str, float]:
        """
        Calculate similarity score for each intent based on its utterances.

        Args:
            df: DataFrame with 'Utterance' and 'Intent' columns

        Returns:
            Dictionary mapping intent names to their similarity scores
        """
        intent_scores = {}

        for intent in df['Intent'].unique():
            # get utterances for this intent
            intent_utterances = df[df['Intent'] == intent]['Utterance'].tolist()

            if len(intent_utterances) < 2:
                intent_scores[intent] = 1.0  # if an intent group has only 1 utterance, it gets a similarity score of 1.0 because there's nothing to compare to
                continue

            # calculate TF-IDF vectors
            tfidf_matrix = self.vectorizer.fit_transform(intent_utterances)

            # calculate pairwise similarities between utterance 1 and utterance 2 and so on (see example at the beginning of this colab notebook)
            similarities = cosine_similarity(tfidf_matrix)

            # calculate average similarity
            np.fill_diagonal(similarities, 0) # exclude self-similarity
            avg_similarity = similarities.sum() / (similarities.size - len(similarities))

            intent_scores[intent] = avg_similarity

        return intent_scores

    def find_outliers(self, df: pd.DataFrame) -> List[Tuple[str, str, str, float]]:
        """
        Find utterances that might belong to a different intent.

        Args:
            df: DataFrame with 'Utterance' and 'Intent' columns

        Returns:
            List of tuples (utterance, current_intent, suggested_intent, similarity_score)
        """
        outliers = []

        # create TF-IDF matrix for all utterances
        all_utterances = df['Utterance'].tolist()
        tfidf_matrix = self.vectorizer.fit_transform(all_utterances)

        # calculate similarities for each utterance
        similarities = cosine_similarity(tfidf_matrix)

        for idx, row in df.iterrows():
            current_intent = row['Intent']
            current_utterance = row['Utterance']

            # get indices of utterances in the same intent
            same_intent_indices = df[df['Intent'] == current_intent].index

            # calculate average similarity to same intent utterances
            same_intent_similarities = similarities[idx][same_intent_indices]
            avg_same_intent = np.mean(same_intent_similarities[same_intent_similarities != 1.0])

            # check other intents
            for other_intent in df['Intent'].unique():
                if other_intent == current_intent:
                    continue

                other_intent_indices = df[df['Intent'] == other_intent].index
                other_intent_similarities = similarities[idx][other_intent_indices]
                avg_other_intent = np.mean(other_intent_similarities)

                # if more similar to other intent
                if avg_other_intent > avg_same_intent:
                    outliers.append((
                        current_utterance,
                        current_intent,
                        other_intent,
                        avg_other_intent
                    ))
                    break

        return outliers

def main():
    scorer = IntentSimilarityScorer()

    # calculate intent similarities
    similarities = scorer.calculate_intent_similarities(df_utts)
    print("Intent Coherence Scores:")
    for intent, score in sorted(similarities.items(), key=lambda x: x[1], reverse=True): # print intent scores in descending order
        print(f"{intent}: {score:.3f}")

    # find outliers
    outliers = scorer.find_outliers(df_utts)
    print("\nPotential Outliers (Top 5):")
    # sort outliers by similarity score (descending order)
    outliers_sorted = sorted(outliers, key=lambda x: x[3], reverse=True)

    print("\nPotential Outliers (Top 5):")

    # print only top 5 outliers to not take up too much space, sorted by highest similarity score
    # can also print ALL outliers
    for i, (utterance, current, suggested, score) in enumerate(outliers_sorted[:5]):
        print(f"Utterance: '{utterance}'")
        print(f"Current Intent: {current}")
        print(f"Suggested Intent: {suggested}")
        print(f"Similarity Score: {score:.3f}\n")

if __name__ == "__main__":
    main()

"""**METHOD II: Sentence Embeddings + Cosine Similatrity**"""

pip install sentence-transformers

from sentence_transformers import SentenceTransformer

class IntentSimilarityScorerSBERT:
    def __init__(self, model_name='paraphrase-multilingual-MiniLM-L12-v2', threshold_zscore: float = 2.0):
      # OTHER pretrained models to get sentence embeddings from:
      # 'T-Systems-onsite/cross-en-de-roberta-sentence-transformer' or 'paraphrase-multilingual-MiniLM-L12-v2' or 'distiluse-base-multilingual-cased-v1'
        # load the SBERT model
        self.model = SentenceTransformer(model_name)
        self.threshold_zscore = threshold_zscore

    def calculate_intent_similarities(self, df: pd.DataFrame) -> dict:
        """
        Calculate similarity score for each intent using sentence embeddings from SBERT.
        """
        intent_scores = {}

        for intent in df['Intent'].unique():
            # get the utterances for this intent
            intent_utterances = df[df['Intent'] == intent]['Utterance'].tolist()

            if len(intent_utterances) < 2:
                intent_scores[intent] = 1.0  # if an intent has only 1 utterance, then give it a perfect "similarity score" of 1.0
                continue

            # get sentence embeddings for utterances using SBERT
            embeddings = self.model.encode(intent_utterances) #embeddings are fetched in batches and each batch is all utterances under an intent, so there are 52 intents = 52 batches
            # check the shape of embeddings
            # print("Embedding Shape:", embeddings.shape)  # shape is (N,384) where N is the number of utterances in an intent group

            # calculate pairwise cosine similarity
            similarities = cosine_similarity(embeddings)

            # exclude self-similarity
            np.fill_diagonal(similarities, 0)
            avg_similarity = similarities.sum() / (similarities.size - len(similarities))

            intent_scores[intent] = avg_similarity

        return intent_scores

    def find_outliers(self, df: pd.DataFrame) -> list:
        """
        Find outliers using SBERT embeddings and cosine similarity.
        """
        outliers = []

        # get embeddings for all utterances
        all_utterances = df['Utterance'].tolist()
        embeddings = self.model.encode(all_utterances)

        # calculate cosine similarity matrix
        similarities = cosine_similarity(embeddings)

        for idx, row in df.iterrows():
            current_intent = row['Intent']
            current_utterance = row['Utterance']

            # get indices of utterances in the same intent
            same_intent_indices = df[df['Intent'] == current_intent].index

            # calculate average similarity to same-intent utterances
            same_intent_similarities = similarities[idx][same_intent_indices]
            avg_same_intent = np.mean(same_intent_similarities[same_intent_similarities != 1.0])

            # check other intents
            for other_intent in df['Intent'].unique():
                if other_intent == current_intent:
                    continue

                other_intent_indices = df[df['Intent'] == other_intent].index
                other_intent_similarities = similarities[idx][other_intent_indices]
                avg_other_intent = np.mean(other_intent_similarities)

                # if more similar to another intent
                if avg_other_intent > avg_same_intent:
                    outliers.append((
                        current_utterance,
                        current_intent,
                        other_intent,
                        avg_other_intent
                    ))
                    break

        return outliers

def main():
    scorer = IntentSimilarityScorerSBERT()

    # calculate intent similarities
    similarities = scorer.calculate_intent_similarities(df_utts)
    print("Intent Coherence Scores from SBERT embeddings:")
    for intent, score in sorted(similarities.items(), key=lambda x: x[1], reverse=True): # print intent scores in descending order
        print(f"{intent}: {score:.3f}")

    # find outliers
    outliers = scorer.find_outliers(df_utts)
    print("\nPotential Outliers (Top 5) based on SBERT:")
    # sort outliers by similarity score (descending order)
    outliers_sorted = sorted(outliers, key=lambda x: x[3], reverse=True)

    print("\nPotential Outliers (Top 5) based on SBERT:")

    # print only top 5 outliers, sorted by highest similarity score
    for i, (utterance, current, suggested, score) in enumerate(outliers_sorted[:5]):
        print(f"Utterance: '{utterance}'")
        print(f"Current Intent: {current}")
        print(f"Suggested Intent: {suggested}")
        print(f"Similarity Score: {score:.3f}\n")

if __name__ == "__main__":
    main()

"""**Visualizing SBERT embeddings**"""

scorer = IntentSimilarityScorerSBERT()
# similarities = scorer.calculate_intent_similarities(df_utts)

# generate sentence embeddings for all utterances in the dataset
embedding_list = scorer.model.encode(df_utts['Utterance'].tolist())

# apply UMAP for dimensionality reduction to 2D
reducer = umap.UMAP(n_neighbors=100, n_components=2, metric='cosine')
embeddings_2d = reducer.fit_transform(embedding_list)

# add the 2D embeddings to the dataframe
df_utts['X'] = embeddings_2d[:, 0]
df_utts['Y'] = embeddings_2d[:, 1]

fig = px.scatter(
    df_utts,
    x='X',
    y='Y',
    color='Intent',  # intent is color-annotated BUT because there are so many intents (52), there is a lot of overlap in similar colors
    hover_data={'Utterance': True, 'Intent': True},
    title="SBERT Utterance Embeddings Visualized with UMAP"
)

fig.show()

## Hover over the datapoints in the plot below to see the utterance and intent. A lot of colors look similar (see comment above), so I'd suggest clickling on the legend
## to see each intent's cluster separately if needed. You can also select 2-3 (or more) clusters by clicking on their points in the legend.

"""WRT "Meine name is wieder richtig geoffnet": Which ASR model transcribes this audio to text? This sounds like an incorrect transcription (?), which can influence any further downstream classification (like this intent outlier detection task)."""